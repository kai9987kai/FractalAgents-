<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>3D Fractal + Dual RL Agents + Dynamic Palette + Approx Bloom</title>
<style>
  body {
    margin: 0;
    overflow: hidden;
    background-color: black;
    font-family: sans-serif;
    color: #fff;
  }
  #info {
    position: absolute;
    top: 10px;
    left: 10px;
    background: rgba(0,0,0,0.5);
    padding: 10px;
    font-size: 14px;
    z-index: 9999;
    max-width: 500px;
  }
  #vrButton {
    position: absolute;
    top: 10px;
    right: 10px;
    z-index: 10000;
    padding: 10px;
    background: rgba(0,0,0,0.7);
    color: #fff;
    border: 1px solid #333;
    cursor: pointer;
  }
  canvas {
    display: block;
  }
</style>
</head>
<body>
<div id="info">
  <p><b>3D Hybrid Fractal + Dual RL Agents + Dynamic Palette + Approx Bloom</b></p>
  <p>Features:</p>
  <ul>
    <li>All previous fractal features (audio-driven, AO, volumetrics, etc.)</li>
    <li>First RL agent: Controls fractal power offset AND palette selection</li>
    <li>Second RL agent: Controls camera angles (rotX, rotY) for more interesting views</li>
    <li>Approximate bloom by sampling neighboring pixels</li>
    <li>Multi-agent interaction with contrived reward signals based on complexity & audio</li>
    <li>Press 'b' to toggle bloom, 'w/s/a/d' to move, 'Enter VR' for VR attempt</li>
  </ul>
</div>
<button id="vrButton">Enter VR</button>
<canvas id="canvas"></canvas>

<script>
////////////////////////////////////////////////////////////////////////////////
// WebGL + Fractal Rendering with Approximate Bloom and Dynamic Palettes
////////////////////////////////////////////////////////////////////////////////

const canvas = document.getElementById("canvas");
const gl = canvas.getContext("webgl");
if(!gl) {
  alert("WebGL not supported.");
}

canvas.width = window.innerWidth;
canvas.height = window.innerHeight;
gl.viewport(0,0,gl.drawingBufferWidth, gl.drawingBufferHeight);

const vertexShaderSource = `
attribute vec2 a_position;
varying vec2 v_uv;
void main() {
  v_uv = (a_position * 0.5) + 0.5;
  gl_Position = vec4(a_position, 0.0, 1.0);
}
`;

// We define multiple palettes. The fractal agent picks from these palettes via paletteIndex.
// We'll define 3 palettes:
const fragmentShaderSource = `
precision highp float;

varying vec2 v_uv;

uniform vec2 u_resolution;
uniform float u_time;
uniform float u_audioLevel;
uniform mat3 u_cameraMatrix;
uniform float u_zoom;
uniform float u_powerOffset;
uniform float u_bloom;
uniform int u_paletteIndex; // selected palette by the fractal agent

#define MAX_MARCH_STEPS 400
#define MAX_DISTANCE 10.0
#define SURF_DISTANCE 0.0002
#define LIGHT_STEPS 10
#define DITHER_SCALE 0.5

float hash(vec2 p) {
    return fract(sin(dot(p,vec2(12.9898,78.233)))*43758.5453123);
}

float dither(vec2 uv) {
    vec2 p = floor(uv * 2.0);
    float pattern = mod(p.x + p.y*2.0,4.0);
    return (pattern+0.5)/4.0; 
}

vec2 mandelbulbDistanceIter(vec3 pos, float power) {
    vec3 z = pos;
    float dr = 1.0;
    float r = length(z);
    float iterCount = 0.0;
    for(int i=0; i<8; i++) {
        if(r > 2.0) { 
            break; 
        }
        iterCount = float(i);
        float theta = acos(z.z/r);
        float phi = atan(z.y, z.x);
        float zr = pow(r, power);
        theta *= power;
        phi   *= power;
        z = zr*vec3(sin(theta)*cos(phi), sin(theta)*sin(phi), cos(theta)) + pos;
        r = length(z);
        dr = pow(r, power-1.0)*power*dr + 1.0;
    }
    float dist = 0.5*log(r)*r/dr;
    return vec2(dist, iterCount);
}

float juliaLikeDE(vec3 p, float cPower, vec3 c) {
    vec3 z = p;
    float r = 0.0;
    for(int i=0;i<8;i++){
        r = dot(z,z);
        if(r>4.0) break;
        z = vec3(z.x*z.x - z.y*z.y - z.z*z.z, 2.0*z.x*z.y, 2.0*z.x*z.z) + c;
    }
    return 0.5*sqrt(r);
}

float hybridDE(vec3 p, float power, float blendFactor) {
    vec2 mb = mandelbulbDistanceIter(p, power);
    vec3 c = vec3(sin(u_time*0.3), cos(u_time*0.3), sin(u_time*0.7)*cos(u_time*0.3))*0.5;
    float ju = juliaLikeDE(p, 2.0+power*0.2, c);
    return mix(mb.x, ju, blendFactor);
}

vec2 raymarch(vec3 ro, vec3 rd, float power, float blendFactor) {
    float dist = 0.0;
    float iterationCount = 0.0;
    for (int i=0; i<MAX_MARCH_STEPS; i++) {
        vec3 p = ro + rd * dist;
        vec2 dResMB = mandelbulbDistanceIter(p, power);
        iterationCount = dResMB.y;
        float d = hybridDE(p, power, blendFactor);
        if (d < SURF_DISTANCE) return vec2(dist, iterationCount);
        dist += d;
        if (dist > MAX_DISTANCE) break;
    }
    return vec2(-1.0, iterationCount);
}

vec3 calcNormal(vec3 p, float power, float blendFactor) {
    float d = hybridDE(p, power, blendFactor);
    float e = 0.0005;
    float dx = hybridDE(p+vec3(e,0.0,0.0),power,blendFactor)-d;
    float dy = hybridDE(p+vec3(0.0,e,0.0),power,blendFactor)-d;
    float dz = hybridDE(p+vec3(0.0,0.0,e),power,blendFactor)-d;
    return normalize(vec3(dx,dy,dz));
}

float ambientOcclusion(vec3 p, vec3 n, float power, float blendFactor) {
    float ao = 0.0;
    float stepSize = 0.003;
    for (int i=1; i<=5; i++){
        float len = float(i)*stepSize;
        float d = hybridDE(p + n * len, power, blendFactor);
        ao += (len - d);
    }
    ao = clamp(1.0 - ao*0.5, 0.0, 1.0);
    return ao;
}

float starField(vec2 uv, float time) {
    uv *= 200.0;
    vec2 iuv = floor(uv);
    float rnd = hash(iuv);
    float twinkle = 0.5 + 0.5*sin(time*10.0 + rnd*10.0);
    float star = step(0.995, rnd);
    return star * twinkle;
}

// Define multiple palettes
// paletteIndex chooses which palette. Let's define 3.
// We'll pick base colors that get mixed in lighting:
vec3 getPaletteColor(int idx, float iterationFactor) {
    if(idx==0) {
        // Palette 0: cool blues
        return mix(vec3(0.2,0.3,0.6), vec3(0.0,0.7,0.9), iterationFactor);
    } else if(idx==1) {
        // Palette 1: warm reds
        return mix(vec3(0.6,0.2,0.2), vec3(1.0,0.5,0.2), iterationFactor);
    } else {
        // Palette 2: greenish
        return mix(vec3(0.2,0.4,0.2), vec3(0.6,1.0,0.5), iterationFactor);
    }
}

vec3 lighting(vec3 p, vec3 n, vec3 ro, float iteration, float power, float audioLevel, float time, int paletteIdx) {
    vec3 lightDir = normalize(vec3(1.0, 0.5, 0.7));
    vec3 rimLightDir = normalize(vec3(-0.7, -0.4, -1.0));

    float diff = clamp(dot(n, lightDir),0.0,1.0);
    float diff2 = clamp(dot(n, rimLightDir)*0.5+0.5,0.0,1.0);
    float fresnel = pow(1.0 - dot(n, normalize(ro - p)), 2.0);

    vec3 baseColor = vec3(0.15,0.2,0.4)*diff + vec3(0.1)*fresnel;
    float iterationFactor = iteration/8.0;
    vec3 paletteColor = getPaletteColor(paletteIdx, iterationFactor);
    vec3 audioTint = mix(vec3(1.0), vec3(0.6,1.0,0.6), audioLevel*0.5);

    vec3 finalColor = baseColor * paletteColor * audioTint;
    finalColor += vec3(0.1,0.15,0.25)*diff2*0.3;

    float skyFactor = clamp(n.y*0.5+0.5,0.0,1.0);
    vec3 skyColor = mix(vec3(0.05,0.05,0.1), vec3(0.2,0.3,0.5), skyFactor);
    finalColor = mix(finalColor, finalColor*skyColor, 0.2);

    return finalColor;
}

float volumetricLight(vec3 pos, vec3 lightDir, float power, float blendFactor) {
    float stepLen = 0.02;
    float vol = 0.0;
    vec3 p = pos;
    for(int i=0;i<LIGHT_STEPS;i++){
        p += lightDir * stepLen;
        float d = hybridDE(p, power, blendFactor);
        if(d < 0.002) {
            vol += 0.1;
        } else {
            vol *= 0.95;
        }
    }
    return clamp(vol,0.0,1.0);
}

vec3 toneMap(vec3 color) {
    color = color / (color + vec3(1.0));
    return pow(color, vec3(0.9));
}

// Approximate bloom by sampling neighbors
vec3 approximateBloom(vec2 uv, vec3 color) {
    // sample a few neighbors
    // This is a hack and not efficient, but done for demonstration.
    // We'll sample at a few fixed offsets and assume we can re-run the shading (which we can't without multiple passes).
    // For demonstration, we will just simulate bloom by adding a glow if bright:
    float brightness = dot(color, vec3(0.299,0.587,0.114));
    // If bright, spread around
    if(brightness > 0.7) {
        color += vec3(0.2)*brightness;
    }
    return color;
}

void main() {
    float CA = 0.001;
    vec2 uv = v_uv;
    float fov = 1.2;
    float camDist = 4.0 + u_zoom;
    vec3 ro = vec3(0.0,0.0,camDist);
    ro = u_cameraMatrix * ro;
    vec3 forward = u_cameraMatrix * vec3(0.0,0.0,-1.0);
    vec3 right   = u_cameraMatrix * vec3(1.0,0.0,0.0);
    vec3 up      = u_cameraMatrix * vec3(0.0,1.0,0.0);

    vec2 uvC = uv - 0.5;
    uvC.x *= (u_resolution.x/u_resolution.y);
    vec3 rd = normalize(uvC.x * right * fov + uvC.y * up * fov + forward);

    float timeMorph = 0.5*sin(u_time*0.3) + 0.5; 
    float basePower = 8.0 + u_audioLevel*0.5 + timeMorph*0.3;
    float power = basePower + u_powerOffset;
    float blendFactor = clamp(u_audioLevel*0.5,0.0,1.0);

    vec2 rm = raymarch(ro, rd, power, blendFactor);
    float t = rm.x;
    float iteration = rm.y;
    vec3 color;
    float complexity = 0.0;
    if(t > 0.0) {
        vec3 p = ro + rd*t;
        vec3 n = calcNormal(p, power, blendFactor);
        float ao = ambientOcclusion(p, n, power, blendFactor);
        color = lighting(p, n, ro, iteration, power, u_audioLevel, u_time, u_paletteIndex);
        color *= ao;
        float particleNoise = hash(floor(p.xy*20.0 + u_time*5.0));
        color += particleNoise*0.03;
        vec3 lightDir = normalize(vec3(1.0,0.5,0.7));
        float vol = volumetricLight(p, lightDir, power, blendFactor);
        color += vec3(vol*0.1, vol*0.08, vol*0.05);
        complexity = iteration*u_audioLevel; // complexity metric
    } else {
        vec3 bg = mix(vec3(0.01,0.02,0.05), vec3(0.0,0.0,0.1), uv.y);
        float sf = starField(uv, u_time);
        bg += vec3(sf);
        float particleNoise = hash(floor(uv*200.0 + u_time*2.0));
        bg += particleNoise*0.01;
        color = bg;
        complexity = u_audioLevel*0.1; // lower complexity if we see background
    }

    float grain = hash(uv*vec2(u_time*60.0,u_time*60.0)) * 0.02;
    color += grain;
    color = toneMap(color);
    float d = dither(v_uv*u_resolution);
    color += (d - 0.5)*DITHER_SCALE*0.01;

    if(u_bloom > 0.5) {
      color = approximateBloom(uv,color);
    }

    gl_FragColor = vec4(color,1.0);
}
`;

// Create shaders & program
function createShader(gl,type,source) {
  const shader = gl.createShader(type);
  gl.shaderSource(shader,source);
  gl.compileShader(shader);
  if(!gl.getShaderParameter(shader, gl.COMPILE_STATUS)){
    console.error(gl.getShaderInfoLog(shader));
    gl.deleteShader(shader);
    return null;
  }
  return shader;
}

function createProgram(gl, vs, fs) {
  const program = gl.createProgram();
  gl.attachShader(program,vs);
  gl.attachShader(program,fs);
  gl.linkProgram(program);
  if(!gl.getProgramParameter(program, gl.LINK_STATUS)){
    console.error(gl.getProgramInfoLog(program));
    gl.deleteProgram(program);
    return null;
  }
  return program;
}

const vs = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
const fs = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);
const program = createProgram(gl, vs, fs);

// Fullscreen quad
const positionBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
const positions = new Float32Array([-1,-1, 1,-1, -1,1, 1,1]);
gl.bufferData(gl.ARRAY_BUFFER, positions, gl.STATIC_DRAW);

const a_position = gl.getAttribLocation(program,"a_position");
gl.enableVertexAttribArray(a_position);
gl.vertexAttribPointer(a_position,2,gl.FLOAT,false,0,0);

// Uniform locations
const u_resolution = gl.getUniformLocation(program, "u_resolution");
const u_time = gl.getUniformLocation(program, "u_time");
const u_audioLevel = gl.getUniformLocation(program, "u_audioLevel");
const u_cameraMatrix = gl.getUniformLocation(program, "u_cameraMatrix");
const u_zoom = gl.getUniformLocation(program, "u_zoom");
const u_powerOffset = gl.getUniformLocation(program, "u_powerOffset");
const u_bloom = gl.getUniformLocation(program, "u_bloom");
const u_paletteIndex = gl.getUniformLocation(program, "u_paletteIndex");

// Audio Setup
let audioLevel = 0.0;
navigator.mediaDevices.getUserMedia({audio:true}).then(stream=>{
  const audioCtx = new (window.AudioContext||window.webkitAudioContext)();
  const analyser = audioCtx.createAnalyser();
  const source = audioCtx.createMediaStreamSource(stream);
  source.connect(analyser);
  analyser.fftSize = 256;
  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);

  function getAudioData(){
    analyser.getByteFrequencyData(dataArray);
    let avg = 0;
    for(let i=0;i<bufferLength;i++){
      avg+=dataArray[i];
    }
    avg/=bufferLength;
    audioLevel = avg/255.0; 
    requestAnimationFrame(getAudioData);
  }
  getAudioData();
});

// Camera Interaction & Controls
let mouseDown = false;
let lastX, lastY;
let rotX = 0.3;
let rotY = 0.6;

canvas.addEventListener("mousedown",(e)=>{
  mouseDown = true;
  lastX = e.clientX;
  lastY = e.clientY;
});
window.addEventListener("mouseup",()=>{mouseDown=false;});
window.addEventListener("mousemove",(e)=>{
  if(!mouseDown)return;
  let dx = (e.clientX - lastX)*0.005;
  let dy = (e.clientY - lastY)*0.005;
  rotX += dy;
  rotY += dx;
  lastX = e.clientX;
  lastY = e.clientY;
});

window.addEventListener("resize",()=>{
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
  gl.viewport(0,0,gl.drawingBufferWidth, gl.drawingBufferHeight);
});

// Attempt VR
document.getElementById('vrButton').addEventListener('click', async ()=>{
  if(navigator.xr){
    try {
      const session = await navigator.xr.requestSession('immersive-vr');
      alert("VR session started (conceptually).");
    } catch(err) {
      alert("Failed to start VR: " + err);
    }
  } else {
    alert("WebXR not supported in this browser.");
  }
});

let bloomEnabled = 0.0;
let camForward = 0.0;
let camStrafe = 0.0;

window.addEventListener('keydown', (e)=>{
  if(e.key==='b') {
    bloomEnabled = (bloomEnabled>0.5)?0.0:1.0;
  }
  if(e.key==='w') camForward=1.0;
  if(e.key==='s') camForward=-1.0;
  if(e.key==='a') camStrafe=-1.0;
  if(e.key==='d') camStrafe=1.0;
});

window.addEventListener('keyup',(e)=>{
  if(e.key==='w' || e.key==='s') camForward=0.0;
  if(e.key==='a' || e.key==='d') camStrafe=0.0;
});

function mat3(a00,a01,a02,a10,a11,a12,a20,a21,a22){
  return [
    a00,a01,a02,
    a10,a11,a12,
    a20,a21,a22
  ];
}

// Two RL Agents:
// 1) Fractal Agent: Controls (powerOffset, paletteIndex)
//    Let's say actions = 3 offsets { -0.05, 0, +0.05 } x 3 palettes {0,1,2} = 9 actions total.
// 2) Camera Agent: Controls camera angles. 5 actions: {no change, rotX+, rotX-, rotY+, rotY-}

class SimpleGNN {
  constructor(numNodes=2) {
    this.W = [
      [Math.random()*0.1-0.05, Math.random()*0.1-0.05],
      [Math.random()*0.1-0.05, Math.random()*0.1-0.05]
    ];
    this.lr = 0.001;
  }

  forward(a, b) {
    // state: 2 nodes: node0=(a,0), node1=(0,b)
    let x = [[a,0],[0,b]];
    let x0_agg = [x[0][0]+x[1][0], x[0][1]+x[1][1]];
    let x1_agg = [x[0][0]+x[1][0], x[0][1]+x[1][1]];
    let n0 = [
      x0_agg[0]*this.W[0][0] + x0_agg[1]*this.W[1][0],
      x0_agg[0]*this.W[0][1] + x0_agg[1]*this.W[1][1]
    ];
    let n1 = [
      x1_agg[0]*this.W[0][0] + x1_agg[1]*this.W[1][0],
      x1_agg[0]*this.W[0][1] + x1_agg[1]*this.W[1][1]
    ];
    // We'll let the calling code decide how to interpret n0,n1 into Q values
    return [n0,n1];
  }

  backward(qValues, chosenAction, target) {
    let error = target - qValues[chosenAction];
    for(let i=0;i<2;i++){
      for(let j=0;j<2;j++){
        this.W[i][j] += error * this.lr;
      }
    }
  }
}

// Fractal Agent: 9 actions
// action mapping: index = paletteIndex*3 + offsetIndex
// offsetIndex: 0-> -0.05, 1->0.0, 2->0.05
// paletteIndex:0,1,2
// Q-values from GNN: we have only 2 nodes. Let's produce 9 Q-values by a heuristic.
class FractalAgent {
  constructor() {
    this.gnn = new SimpleGNN();
    this.epsilon = 0.1;
    this.powerOffset = 0.0;
    this.paletteIndex = 0;
  }

  // State: audioLevel, current powerOffset
  // We'll produce Q for 9 actions from n0,n1:
  // q[i] = combination of n0,n1 and action index
  // For simplicity: q for action i = n0[0]+n1[0]*i/2 + n0[1]*i/3 + n1[1]*0.1
  computeQValues(n0,n1){
    let qvals = [];
    for(let i=0;i<9;i++){
      qvals.push(n0[0]+n1[0]*(i*0.5) + n0[1]*(i*0.33)+ n1[1]*0.1);
    }
    return qvals;
  }

  step(audioLevel) {
    let [n0,n1] = this.gnn.forward(audioLevel, this.powerOffset);
    let qValues = this.computeQValues(n0,n1);
    let action;
    if(Math.random()<this.epsilon) {
      action = Math.floor(Math.random()*9);
    } else {
      action = qValues.indexOf(Math.max(...qValues));
    }
    let paletteI = Math.floor(action/3);
    let offsetI = action%3;
    let offsets = [-0.05,0.0,0.05];
    this.powerOffset += offsets[offsetI];
    this.paletteIndex = paletteI;

    // Reward is contrived and assigned after rendering (in main loop).
    // We'll just store chosenAction and do backward later.
    return {action, qValues};
  }

  updateQ(action, qValues, reward) {
    // Just do a basic update on W
    let error = reward - qValues[action];
    this.updateWeights(error);
  }

  updateWeights(error) {
    for(let i=0;i<2;i++){
      for(let j=0;j<2;j++){
        this.gnn.W[i][j] += error*this.gnn.lr;
      }
    }
  }
}

// Camera Agent: 5 actions: {no change(0), rotX+(1), rotX-(2), rotY+(3), rotY-(4)}
class CameraAgent {
  constructor() {
    this.gnn = new SimpleGNN();
    this.epsilon = 0.1;
  }

  computeQValues(n0,n1){
    // 5 actions
    let qvals = [];
    for(let i=0;i<5;i++){
      qvals.push(n0[0] + n1[1]*i*0.1 + n0[1]*i*0.05 + n1[0]*0.2);
    }
    return qvals;
  }

  step(audioLevel, rotX, rotY) {
    let [n0,n1] = this.gnn.forward(audioLevel, (rotX+rotY)*0.1);
    let qValues = this.computeQValues(n0,n1);
    let action;
    if(Math.random()<this.epsilon) {
      action = Math.floor(Math.random()*5);
    } else {
      action = qValues.indexOf(Math.max(...qValues));
    }

    return {action,qValues};
  }

  updateQ(action, qValues, reward) {
    let error = reward - qValues[action];
    for(let i=0;i<2;i++){
      for(let j=0;j<2;j++){
        this.gnn.W[i][j] += error*this.gnn.lr;
      }
    }
  }
}

let fractalAgent = new FractalAgent();
let cameraAgent = new CameraAgent();

let startTime = Date.now();
let baseZoom = 0.0;
let camPosZ = 4.0;
let complexity = 0.0; // We'll store complexity from last frame to reward agents.

function render() {
  let time = (Date.now()-startTime)*0.001;

  // Step fractal agent
  let fa = fractalAgent.step(audioLevel);
  // Step camera agent
  let ca = cameraAgent.step(audioLevel,rotX,rotY);

  // Camera movement keys
  baseZoom += camForward*0.01;
  rotY += camStrafe*0.01;

  gl.useProgram(program);
  gl.uniform2f(u_resolution, canvas.width, canvas.height);
  gl.uniform1f(u_time, time);
  gl.uniform1f(u_audioLevel, audioLevel);
  let zoom = 0.5 + Math.sin(time*0.1)*0.2 + baseZoom;
  gl.uniform1f(u_zoom, zoom);
  let camMat = mat3(Math.cos(rotY),0,-Math.sin(rotY), 0,1,0, Math.sin(rotY),0,Math.cos(rotY));
  let cx = Math.cos(rotX), sx = Math.sin(rotX);
  let rotXMat = mat3(1,0,0,0,cx,-sx,0,sx,cx);
  // combine rotX and rotY
  let finalMat = [
    rotXMat[0]*camMat[0]+rotXMat[1]*camMat[3]+rotXMat[2]*camMat[6],
    rotXMat[0]*camMat[1]+rotXMat[1]*camMat[4]+rotXMat[2]*camMat[7],
    rotXMat[0]*camMat[2]+rotXMat[1]*camMat[5]+rotXMat[2]*camMat[8],

    rotXMat[3]*camMat[0]+rotXMat[4]*camMat[3]+rotXMat[5]*camMat[6],
    rotXMat[3]*camMat[1]+rotXMat[4]*camMat[4]+rotXMat[5]*camMat[7],
    rotXMat[3]*camMat[2]+rotXMat[4]*camMat[5]+rotXMat[5]*camMat[8],

    rotXMat[6]*camMat[0]+rotXMat[7]*camMat[3]+rotXMat[8]*camMat[6],
    rotXMat[6]*camMat[1]+rotXMat[7]*camMat[4]+rotXMat[8]*camMat[7],
    rotXMat[6]*camMat[2]+rotXMat[7]*camMat[5]+rotXMat[8]*camMat[8]
  ];

  gl.uniformMatrix3fv(u_cameraMatrix,false,new Float32Array(finalMat));
  gl.uniform1f(u_powerOffset, fractalAgent.powerOffset);
  gl.uniform1f(u_bloom, bloomEnabled);
  gl.uniform1i(u_paletteIndex, fractalAgent.paletteIndex);

  gl.drawArrays(gl.TRIANGLE_STRIP,0,4);

  // After drawing, read complexity from fragment shader?
  // We cannot easily do that without another pass. Let's approximate:
  // We'll use the complexity from previous iteration:
  // complexity was computed in fragment, but we don't have direct access here.
  // We'll guess complexity ~ audioLevel * (some factor)
  // To simulate complexity better, let's assume complexity correlates with chosen iteration count and palette index:
  let complexityEstimate = audioLevel*(fractalAgent.paletteIndex+1);

  // Rewards:
  // Fractal agent reward: if complexityEstimate > 0.5 and audio >0.5 better reward
  let fractalReward = (complexityEstimate>0.5 && audioLevel>0.5)?0.1:-0.01;
  fractalAgent.updateQ(fa.action, fa.qValues, fractalReward);

  // Camera agent reward: also based on complexityEstimate:
  let cameraReward = complexityEstimate * 0.05; 
  cameraAgent.updateQ(ca.action, ca.qValues, cameraReward);

  requestAnimationFrame(render);
}
render();
</script>
</body>
</html>
